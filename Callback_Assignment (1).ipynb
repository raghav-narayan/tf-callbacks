{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Callback Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQDRNrY2NCXf"
      },
      "source": [
        "<pre>\n",
        "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
        "\n",
        "2. Code the model to classify data like below image\n",
        "\n",
        "<img src='https://i.imgur.com/33ptOFy.png'>\n",
        "\n",
        "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
        "\n",
        "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
        "\n",
        "5. you have to decay learning based on below conditions \n",
        "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
        "               learning rate by 10%. \n",
        "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
        "        \n",
        "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
        "\n",
        "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
        "\n",
        "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
        "\n",
        "9. use cross entropy as loss function\n",
        "\n",
        "10. Try the architecture params as given below. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "bTTAy8-q34fw",
        "outputId": "5bfd4d19-647e-4106-c0c6-ea903c3ccad5"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c78c5d5-e6f4-4d58-9b77-8f90908e359e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6c78c5d5-e6f4-4d58-9b77-8f90908e359e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64J8kes-5iih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "01937761-aa19-4b35-b736-673735995dbe"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z6xMByOl4SLZ",
        "outputId": "5662bfc2-41ee-40e1-822d-2499fab5bc36"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "#import tensorflow as tf\r\n",
        "#enabled to get instant output. if you don't need, you can use session concept which was dicussed in lecture videos. \r\n",
        "#tf.enable_eager_execution()\r\n",
        "df = pd.read_csv('data.csv')\r\n",
        "print(df.head())\r\n",
        "X =df[['f1','f2']]\r\n",
        "y = df['label']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         f1        f2  label\n",
            "0  0.450564  1.074305    0.0\n",
            "1  0.085632  0.967682    0.0\n",
            "2  0.117326  0.971521    1.0\n",
            "3  0.982179 -0.380408    0.0\n",
            "4 -0.720352  0.955850    0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzlh6Psbo8NB"
      },
      "source": [
        "class LossHistory(tf.keras.callbacks.Callback):\r\n",
        "    \r\n",
        "    def on_train_begin(self, logs={}):\r\n",
        "        ## on begin of training, we are creating a instance varible called history\r\n",
        "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\r\n",
        "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\r\n",
        "        \r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        ## on end of each epoch, we will get logs and update the self.history dict\r\n",
        "        self.history['loss'].append(logs.get('loss'))\r\n",
        "        self.history['acc'].append(logs.get('acc'))\r\n",
        "        if logs.get('val_loss', -1) != -1:\r\n",
        "            self.history['val_loss'].append(logs.get('val_loss'))\r\n",
        "        if logs.get('val_acc', -1) != -1:\r\n",
        "            self.history['val_acc'].append(logs.get('val_acc'))\r\n",
        "        #self.lr.append(step_decay(len(self.history['loss'])))\r\n",
        "              \r\n",
        "history_own=LossHistory()            "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbn0g3r5xV3"
      },
      "source": [
        "import math\r\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "# Decrease LR based on condtion 2 -- decrease by 5% every 3 epochs\r\n",
        "def step_decay(epoch,lr):\r\n",
        "  if (epoch+1)%3==0:\r\n",
        "    lr = 0.95*lr\r\n",
        "  return lr\r\n",
        "lrate = LearningRateScheduler(step_decay)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci4X2e1tDdJj"
      },
      "source": [
        "# Terminate training if either weights or loss value has Nan\r\n",
        "class TerminateNan(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self,epoch,log={}):\r\n",
        "    loss = logs.get('loss')\r\n",
        "    if loss not in None:\r\n",
        "      if np.isnan(loss) or np.isinf(loss):\r\n",
        "        print('Invalid loss and terminated at epoch {}'.format(epoch))\r\n",
        "        self.model.stop_training = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkJrRCnc0xRR"
      },
      "source": [
        "#Custom callback to implment f1score for at end of each epoch\r\n",
        "class Metrics(tf.keras.callbacks.Callback):\r\n",
        "  def __init__(self,validation):\r\n",
        "    super(Metrics,self).__init__()\r\n",
        "    self.validation = validation\r\n",
        "  def on_train_begin(self,logs={}):\r\n",
        "    self.val_f1s = []\r\n",
        "  def on_epoch_end(self,epoch,logs={}):\r\n",
        "    val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()\r\n",
        "    val_target = self.validation[1]\r\n",
        "    val_f1_score = f1_score(val_target,val_predict,average='micro')\r\n",
        "\r\n",
        "    self.val_f1s.append(round(val_f1_score,6))\r\n",
        "    print('--f1_score: {0}'.format(round(val_f1_score,2)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDOBt-5MyiFn"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\r\n",
        "#Implementing AUC score as a callback\r\n",
        "class AUC_Callback(tf.keras.callbacks.Callback):\r\n",
        "  def __init__(self,validation):\r\n",
        "    super(AUC_Callback,self).__init__()\r\n",
        "    self.validation = validation\r\n",
        "  def on_train_begin(self,logs={}):\r\n",
        "    self.val_auc = []\r\n",
        "  def on_epoch_end(self,epoch,logs={}):\r\n",
        "    #print(self.model)\r\n",
        "    val_predict =(self.model.predict(self.validation[0]))\r\n",
        "    #print(val_predict[10:20])\r\n",
        "    val_target = self.validation[1]\r\n",
        "    val_auc = roc_auc_score(val_target,val_predict)\r\n",
        "\r\n",
        "    self.val_auc.append(round(val_auc,6))\r\n",
        "    print('--AUC: {0}'.format(round(val_auc,2)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9A4sHHe0x3f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "#Standardizing the inputs\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "sc = StandardScaler()\r\n",
        "X_train = sc.fit_transform(X_train)\r\n",
        "X_test = sc.transform (X_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSa0rNXK7j2T"
      },
      "source": [
        "term = TerminateNan()\r\n",
        "from tensorflow.keras.layers import Dense,Input,Activation,Dropout\r\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciCBUND4iVmP"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "earlystop = EarlyStopping(monitor='val_acc', patience=2, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2y9hWeD7ww0"
      },
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\r\n",
        "reduce = ReduceLROnPlateau(monitor='val_acc',factor=0.9,patience=1,verbose=1)\r\n",
        "# To reduce learning rate based on condition 1 --- reduce lr by 10% if 'val_acc' is not improving\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB9J96F797bx"
      },
      "source": [
        "# !pip install -q tf-nightly-2.0-preview\r\n",
        "# if you want to use the tf2.0 please uncomment the above line\r\n",
        "# Load the TensorBoard notebook extension\r\n",
        "\r\n",
        "# there are other ways of doing this: https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/ you can try this way also\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoXY441tRr48"
      },
      "source": [
        "# Clear any logs from previous runs\r\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5t_yAaZRsYT"
      },
      "source": [
        "import datetime\r\n",
        "log_dir = \"logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeLgvn-m7scu"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "77s6p4IC7kY_",
        "outputId": "fd3c56e3-1051-4192-f284-3e02632ddb7d"
      },
      "source": [
        "auc = AUC_Callback(validation=(X_test,y_test))\r\n",
        "input_layer = Input(shape=(2,))\r\n",
        "dense1 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.,seed=30))(input_layer)\r\n",
        "dense2 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense1)\r\n",
        "dense3 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense2)\r\n",
        "dense4 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense3)\r\n",
        "dense5 = Dense(5,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense4)\r\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense5)\r\n",
        "optimizer  = tf.keras.optimizers.SGD(learning_rate=0.1,momentum=0.9)\r\n",
        "model = Model(inputs= input_layer,outputs=output)\r\n",
        "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "callback_list=[Metrics(validation=(X_test,y_test)),auc,earlystop,history_own,lrate,reduce,tensorboard_callback]\r\n",
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=128,callbacks=callback_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v1.py:59: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "  128/16000 [..............................] - ETA: 12s - loss: 0.9445 - acc: 0.5781WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_begin` time: 0.0019s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0018s). Check your callbacks.\n",
            " 2176/16000 [===>..........................] - ETA: 0s - loss: 0.8071 - acc: 0.5028 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13696/16000 [========================>.....] - ETA: 0s - loss: 0.7159 - acc: 0.5004--f1_score: 0.49\n",
            "--AUC: 0.49\n",
            "16000/16000 [==============================] - 1s 47us/sample - loss: 0.7130 - acc: 0.5004 - val_loss: 0.6957 - val_acc: 0.4933\n",
            "Epoch 2/10\n",
            "11264/16000 [====================>.........] - ETA: 0s - loss: 0.6945 - acc: 0.4975--f1_score: 0.49\n",
            "--AUC: 0.52\n",
            "\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.09000000134110452.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6947 - acc: 0.4947 - val_loss: 0.6961 - val_acc: 0.4925\n",
            "Epoch 3/10\n",
            "12544/16000 [======================>.......] - ETA: 0s - loss: 0.6941 - acc: 0.4971--f1_score: 0.51\n",
            "--AUC: 0.5\n",
            "16000/16000 [==============================] - 0s 21us/sample - loss: 0.6947 - acc: 0.4923 - val_loss: 0.6963 - val_acc: 0.5098\n",
            "Epoch 4/10\n",
            "12288/16000 [======================>.......] - ETA: 0s - loss: 0.6951 - acc: 0.4927--f1_score: 0.49\n",
            "--AUC: 0.49\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.07695000171661377.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6950 - acc: 0.4906 - val_loss: 0.6953 - val_acc: 0.4935\n",
            "Epoch 5/10\n",
            "13312/16000 [=======================>......] - ETA: 0s - loss: 0.6941 - acc: 0.5013--f1_score: 0.5\n",
            "--AUC: 0.5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.06925499886274337.\n",
            "16000/16000 [==============================] - 0s 20us/sample - loss: 0.6940 - acc: 0.5022 - val_loss: 0.6952 - val_acc: 0.4985\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5bb6b38be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GL5e-P_9IAi"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTq6BgYc-o3U"
      },
      "source": [
        "\r\n",
        "import datetime\r\n",
        "\r\n",
        "log_dir = \"logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tensorboard_callback2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "V8rCjH0E9HBm",
        "outputId": "337e072c-cd69-45e6-d3f6-16efdc1dfecc"
      },
      "source": [
        "auc = AUC_Callback(validation=(X_test,y_test))\r\n",
        "input_layer = Input(shape=(2,))\r\n",
        "dense1 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.,seed=30))(input_layer)\r\n",
        "dense2 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense1)\r\n",
        "dense3 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense2)\r\n",
        "dense4 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense3)\r\n",
        "dense5 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense4)\r\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.,maxval=1.))(dense5)\r\n",
        "optimizer  = tf.keras.optimizers.SGD(learning_rate=0.1,momentum=0.9)\r\n",
        "model = Model(inputs= input_layer,outputs=output)\r\n",
        "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "callback_list=[Metrics(validation=(X_test,y_test)),auc,earlystop,history_own,lrate,reduce,tensorboard_callback2]\r\n",
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=128,callbacks=callback_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v1.py:59: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "  128/16000 [..............................] - ETA: 13s - loss: 18.0577 - acc: 0.4688WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_begin` time: 0.0019s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.\n",
            " 2176/16000 [===>..........................] - ETA: 0s - loss: 1.7174 - acc: 0.5009  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13696/16000 [========================>.....] - ETA: 0s - loss: 0.8561 - acc: 0.5014--f1_score: 0.5\n",
            "--AUC: 0.5\n",
            "16000/16000 [==============================] - 1s 47us/sample - loss: 0.8326 - acc: 0.5038 - val_loss: 0.6941 - val_acc: 0.4992\n",
            "Epoch 2/10\n",
            "11904/16000 [=====================>........] - ETA: 0s - loss: 0.6937 - acc: 0.5060--f1_score: 0.5\n",
            "--AUC: 0.5\n",
            "\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.09000000134110452.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6938 - acc: 0.5069 - val_loss: 0.6935 - val_acc: 0.4992\n",
            "Epoch 3/10\n",
            "15104/16000 [===========================>..] - ETA: 0s - loss: 0.6939 - acc: 0.4976--f1_score: 0.5\n",
            "--AUC: 0.5\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.07695000171661377.\n",
            "16000/16000 [==============================] - 0s 24us/sample - loss: 0.6938 - acc: 0.4989 - val_loss: 0.6938 - val_acc: 0.4992\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c1eb4ebe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uNMlpecCjXB8",
        "outputId": "b8c882e6-b9fa-4f74-8e83-3231bd3c6c0a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 15        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 141\n",
            "Trainable params: 141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41Y3TFENCXk"
      },
      "source": [
        "<pre>\n",
        "<b>Model-1</b>\n",
        "<pre>\n",
        "1. Use tanh as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use RandomUniform(0,1) as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-2</b>\n",
        "<pre>\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use RandomUniform(0,1) as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-3</b>\n",
        "<pre>\n",
        "1. Use relu as an activation for every layer except output layer.\n",
        "2. use SGD with momentum as optimizer.\n",
        "3. use he_uniform() as initilizer.\n",
        "3. Analyze your output and training process. \n",
        "</pre>\n",
        "</pre>\n",
        "<pre>\n",
        "<b>Model-4</b>\n",
        "<pre>\n",
        "1. Try with any values to get better accuracy/f1 score.  \n",
        "</pre>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9jTmhCj7V2"
      },
      "source": [
        "# Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVeu9Lb60bvC"
      },
      "source": [
        "import datetime\r\n",
        "log_dir = \"logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-skF56-R7kqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cd0444a1-f1ca-416b-9acc-d9dafaf6cb61"
      },
      "source": [
        "auc = AUC_Callback(validation=(X_test,y_test))\r\n",
        "input_layer = Input(shape=(2,))\r\n",
        "dense1 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(input_layer)\r\n",
        "dense2 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense1)\r\n",
        "dense3 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense2)\r\n",
        "dense4 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense3)\r\n",
        "dense5 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense4)\r\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.he_normal())(dense5)\r\n",
        "optimizer  = tf.keras.optimizers.SGD(learning_rate=0.1,momentum=0.9)\r\n",
        "model = Model(inputs= input_layer,outputs=output)\r\n",
        "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "callback_list=[Metrics(validation=(X_test,y_test)),auc,earlystop,history_own,lrate,reduce,tensorboard_callback]\r\n",
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=128,callbacks=callback_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "  128/16000 [..............................] - ETA: 13s - loss: 0.8072 - acc: 0.5781WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_begin` time: 0.0018s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.\n",
            " 2816/16000 [====>.........................] - ETA: 0s - loss: 0.7034 - acc: 0.5156 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14336/16000 [=========================>....] - ETA: 0s - loss: 0.6792 - acc: 0.5670--f1_score: 0.64\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 1s 46us/sample - loss: 0.6742 - acc: 0.5751 - val_loss: 0.6439 - val_acc: 0.6355\n",
            "Epoch 2/10\n",
            "11520/16000 [====================>.........] - ETA: 0s - loss: 0.6111 - acc: 0.6638--f1_score: 0.64\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 0s 23us/sample - loss: 0.6165 - acc: 0.6621 - val_loss: 0.6356 - val_acc: 0.6447\n",
            "Epoch 3/10\n",
            "11520/16000 [====================>.........] - ETA: 0s - loss: 0.6115 - acc: 0.6616--f1_score: 0.64\n",
            "--AUC: 0.7\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.08549999892711639.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6098 - acc: 0.6654 - val_loss: 0.6338 - val_acc: 0.6370\n",
            "Epoch 4/10\n",
            "11520/16000 [====================>.........] - ETA: 0s - loss: 0.6084 - acc: 0.6688--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6070 - acc: 0.6696 - val_loss: 0.6296 - val_acc: 0.6538\n",
            "Epoch 5/10\n",
            "12928/16000 [=======================>......] - ETA: 0s - loss: 0.6088 - acc: 0.6675--f1_score: 0.66\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 0s 21us/sample - loss: 0.6062 - acc: 0.6711 - val_loss: 0.6248 - val_acc: 0.6560\n",
            "Epoch 6/10\n",
            "12544/16000 [======================>.......] - ETA: 0s - loss: 0.6042 - acc: 0.6683--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.07310250028967857.\n",
            "16000/16000 [==============================] - 0s 23us/sample - loss: 0.6055 - acc: 0.6672 - val_loss: 0.6306 - val_acc: 0.6455\n",
            "Epoch 7/10\n",
            "11136/16000 [===================>..........] - ETA: 0s - loss: 0.6079 - acc: 0.6635--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.06579225361347199.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6085 - acc: 0.6639 - val_loss: 0.6252 - val_acc: 0.6530\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f444ebe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kL6X2eTBs-7B",
        "outputId": "96785c3e-d0d5-43b2-ce10-280d94a7fa52"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 15        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 141\n",
            "Trainable params: 141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzTIYS-Yn7P3"
      },
      "source": [
        "# Model 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plWGyEzG-nL1"
      },
      "source": [
        "import datetime\r\n",
        "log_dir = \"logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EFal6-45ZEuB",
        "outputId": "d68cd7c4-cb8a-4646-e0a7-dab351df82e1"
      },
      "source": [
        "#Adding dropout and using adam optimizer\r\n",
        "auc = AUC_Callback(validation=(X_test,y_test))\r\n",
        "input_layer = Input(shape=(2,))\r\n",
        "dense1 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(input_layer)\r\n",
        "dense2 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense1)\r\n",
        "dense3 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense2)\r\n",
        "dense4 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense3)\r\n",
        "dense5 = Dense(5,activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(dense4)\r\n",
        "output = Dense(1,activation='sigmoid',kernel_initializer=tf.keras.initializers.he_normal())(dense5)\r\n",
        "optimizer  = tf.keras.optimizers.Adam(learning_rate=0.01)\r\n",
        "model = Model(inputs= input_layer,outputs=output)\r\n",
        "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "callback_list=[Metrics(validation=(X_test,y_test)),auc,earlystop,history_own,lrate,reduce,tensorboard_callback]\r\n",
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=11,batch_size=128,callbacks=callback_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/11\n",
            "  128/16000 [..............................] - ETA: 14s - loss: 0.8995 - acc: 0.4609WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_begin` time: 0.0019s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
            " 2432/16000 [===>..........................] - ETA: 0s - loss: 0.7156 - acc: 0.4955 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13824/16000 [========================>.....] - ETA: 0s - loss: 0.6885 - acc: 0.5407--f1_score: 0.61\n",
            "--AUC: 0.63\n",
            "16000/16000 [==============================] - 1s 49us/sample - loss: 0.6846 - acc: 0.5496 - val_loss: 0.6628 - val_acc: 0.6095\n",
            "Epoch 2/11\n",
            "11904/16000 [=====================>........] - ETA: 0s - loss: 0.6387 - acc: 0.6394--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6339 - acc: 0.6452 - val_loss: 0.6243 - val_acc: 0.6515\n",
            "Epoch 3/11\n",
            "11264/16000 [====================>.........] - ETA: 0s - loss: 0.6070 - acc: 0.6662--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "16000/16000 [==============================] - 0s 23us/sample - loss: 0.6083 - acc: 0.6641 - val_loss: 0.6222 - val_acc: 0.6520\n",
            "Epoch 4/11\n",
            "11392/16000 [====================>.........] - ETA: 0s - loss: 0.6037 - acc: 0.6631--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.008549999725073577.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6033 - acc: 0.6639 - val_loss: 0.6300 - val_acc: 0.6488\n",
            "Epoch 5/11\n",
            "11648/16000 [====================>.........] - ETA: 0s - loss: 0.6016 - acc: 0.6673--f1_score: 0.65\n",
            "--AUC: 0.71\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.007694999501109123.\n",
            "16000/16000 [==============================] - 0s 22us/sample - loss: 0.6034 - acc: 0.6657 - val_loss: 0.6377 - val_acc: 0.6453\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9079b8aa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jE-oj6HH1Ljs",
        "outputId": "ebeeff90-1547-4707-84e2-c4b838022735"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 15        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 30        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 141\n",
            "Trainable params: 141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}